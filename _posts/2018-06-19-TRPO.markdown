---
layout: post
title:  "Trust Region Policy Optimization"
date:   2018-06-19 12:00:00 -0400
categories: reinforcement-learning
authors: ['surya', 'krishna']
blurb: "TRPO is a model-free algorithm for optimizing policies in reinforcement
       learning by gradient descent. It represents a significant improvement over
       previous methods in its scalability and consequently has enjoyed widespread
       success."
hidden: true
feedback: true
---

(Thank you to Avital Oliver, Nic Ford, Matthew Johnson and George Tucker for contributing to this guide.)

<div class="deps-graph">
  <iframe class="deps" src="/assets/trpo-deps.svg" width="200"></iframe>
  <div>Concepts used in TRPO. Click to navigate.</div>
</div>

# Why

TRPO is a scalable algorithm for optimizing policies in reinforcement learning by
gradient descent. Model-free algorithms such as policy gradient methods do not
require access to a model of the environment and often enjoy better
practical stability. Consequently, while straightforward to apply to new
problems, they have trouble scaling to large, nonlinear policies. TRPO couples
insights from reinforcement learning and optimization theory to develop an
algorithm which, under certain assumptions, provides guarantees for monotonic
improvement.

  <a href="javascript:alert('Coming soon!')" class="colab-root">Colab Notebook
    <span>Soon</span></a>

<br />

# 1 Policy Gradient
  **Motivation**: Policy gradient methods are a class of algorithms that allow
  one to directly optimize the parameters of a policy (whether it is a linear
  function or a complex neural network) by gradient descent. TRPO is one such
  algorithm. In this section, we formalize the notion of Markov Decision Processes (MDP),
  action and state spaces, and on-policy vs off-policy approaches. This leads to
  REINFORCE algorithm, the simplest instantiation of the policy gradient method.

  **Topics**:
  1. Markov Decision Processes.
  2. Continuous action spaces.
  3. On-policy and off-policy.
  4. REINFORCE / likelihood ratio methods.

  **Required Readings**:
  1. Deep RL Course at UC Berkeley (CS 294); Policy Gradient Lecture
      1. [Slides](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf)
      2. [Video](https://www.youtube.com/watch?v=tWNpiNzWuO8&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=4)
  2. David Silver’s course at UCL; Policy Gradient Lecture
      1. [Slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)
      2. [Video](https://www.youtube.com/watch?v=KHZVXao4qXs&index=7&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
  3. Reinforcement Learning by Sutton and Barto, 2nd Edition; pages 265 - 273.
  4. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)

  **Optional Readings**:
  1. [John Schulman introduction at MLSS Cadiz](http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/2016-MLSS-RL.pdf)
  2. [Lecture on Variance Reduction for Policy Gradient](http://rail.eecs.berkeley.edu/deeprlcoursesp17/docs/lec6.pdf)
  3. [Introduction to policy gradient and motivations by Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/)
  4. [Connection Between Importance Sampling and Likelihood Ratio](https://papers.nips.cc/paper/3922-on-a-connection-between-importance-sampling-and-the-likelihood-ratio-policy-gradient.pdf)

  **Questions**:
  1. Remember that at its core, REINFORCE describes an algorithm to compute an
    approximate gradient of the reward with respect to the parameters. Why
    can't we just use gradient descent? What makes the learning procedure
    "non-differentiable"?
     <details><summary>Hint</summary>
     <p>
    Just as in reinforcement learning, when we use stochastic gradient descent, we also compute an estimate of the gradient, which looks something like $$\nabla_{\theta} \mathbb{E}_{x \sim p(x)} [ f_{\theta}(x) ] = \mathbb{E}_{x \sim p(x)} [\nabla_{\theta} f_{\theta}(x)]$$ where we move the gradient into the expectation (an operation made precise by the <a href=""> Leibniz integral rule</a>). In other words, the objective we're trying to take the gradient of is indeed differentiable with respect to the inputs. In reinforcement learning, our objective is non-differentiable -- we actually <b>select</b> an action and act on it. To convince yourself that this isn't something we can differentiate through, write out explicitly the full expansion of the training objective for policy gradient before we move the gradient into the expectation. Is sampling an action really non-differentiable? (spoiler: yes, but we can work around it in various ways, such as using REINFORCE or <a href="https://arxiv.org/abs/1611.01144">other methods</a>).
     </p>
     </details>
  2. Does the REINFORCE gradient estimator resemble maximum likelihood estimation (MLE)?
    Why or why not?
     <details><summary>Solution</summary>
     <p>
     The term \( \log \pi (a | s) \) should look like a very familiar tool in statistical learning: the likelihood function! When we think of what happens when we do MLE, we are trying to maximize the likelihood of \( \log p(D | \theta) \) or as in supervised learning, we try to maximize $$\log p(y_i^* | x_i, \theta).$$ Normally, because we have the true label \( y_i^* \), this paradigm aligns perfectly with what we are ultimately trying to do with MLE. However, this naive strategy of maximizing the likelihood \( \pi(a | s) \) won't work in reinforcement learning, because we do not have a label for the correct action to be taken at a given time step (if we did, we should just do supervised learning!). If we tried doing this, we would find that we would simply maximize the probability of every action; make sure you convince yourself this to be true. Instead, the only (imperfect) evidence we have of good or bad actions is the reward we receive at that time step. Thus, a reasonable thing to do seems like scaling the log-likelihood by how good or bad the action by the reward. Thus, we would then maximize $$r(a, s) \log \pi (a | s).$$ Look familiar? This is just the REINFORCE term in our expectation: $$ \mathbb{E}_{s,a} [ \nabla r(a, s) \log \pi (a | s) ] $$
     </p>
     </details>
  3. In its original formulation, REINFORCE is an on-policy algorithm. Why?
    Can we make REINFORCE work off-policy as well?
     <details><summary>Solution</summary>
     <p>
    We can tell that REINFORCE is on-policy by looking at the expectation a bit closer: $$ \mathbb{E}_{s,a} [ \nabla \log \pi (a | s) r(a, s). ]$$ When we see any expectation in an equation, we should always ask what exactly is the expectation <b>over</b>? In this case, if we expand the expectation, we have: $$\mathbb{E}_{s \sim p_{\theta}(s), a \sim \pi_{\theta}(a|s)} [ \nabla_{\theta} \log \pi_{\theta} (a | s) r(a, s), ]$$ and we see that while the states are being sampled from the empirical state visitation distribution induced by the current policy, and the actions \( a \) are coming directly from the current policy. Because we learn from the current policy, and not some arbitrary policy, REINFORCE is an on-policy. To change REINFORCE to use data, we simply change the sampling distribution to some other policy \( \pi_{\beta} \) and use importance sampling to correct for this disparity. For more details, see <a href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1079&context=cs_faculty_pubs">a classic paper</a> on this subject and <a href="https://arxiv.org/abs/1606.02647">a recent paper</a> with new insights on off-policy learning with policy gradient methods.
     </p>
     </details>
  4. Do policy gradient methods work for discrete and continuous action spaces?
    If not, why not? If so, is there an advantage policy gradient methods
    provide over other RL algorithms, like Q-learning?

<br />

# 2 Variance Reduction and Advantage Estimate
  **Motivation**: One major shortcoming of policy gradient methods is that the
  simplest instantation of REINFORCE suffers from high variance in the gradients
  it computes. This results from the rewards being sparse, only visiting a finite
  set of states, and that we only take one action at each state rather than all actions.
  In order to properly scale, we need to reduce this variance. In this section, we study
  common techniques for reducing variance for REINFORCE: causal approaches,
  baselines, and advantages. Note that TRPO does not introduce new methods for
  variance reduction, but we cover it here for complete understanding.

  **Topics**:
  1. Causality for REINFORCE.
  2. Baselines and control variates.
  2. Advantage estimation.

  **Required Readings**:
  1. Deep RL Course at UC Berkeley (CS 294); Actor-Critic Methods Lecture
      1. [Slides](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf)
      2. [Video](https://www.youtube.com/watch?v=PpVhtJn-iZI&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=5)
  2. [George Tucker’s notes on Variance Reduction](/assets/gjt-var-red-notes.pdf)

  **Optional Readings**:
  1. Reinforcement Learning by Sutton and Barto, 2nd Edition; pages 273 - 275.
  2. [High-dimensional continuous control using generalized advantage estimation](https://arxiv.org/abs/1506.02438)
  3. [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)
  4. [Monte Carlo theory, methods, and examples by Art B. Owen; Chapter 8](https://statweb.stanford.edu/~owen/mc/Ch-var-basic.pdf)
    (in-depth treatment of variance reduction; suitable for independent study).

  **Questions**:
  1. What is the intuition for using advantages instead of rewards as the
    learning signal? Note on terminology: the learning signal is the term that
    we multiply $$\log \pi (a | s)$$ by inside the expectation of REINFORCE.
  2. What are some assumptions we make by using baselines as a variance
    reduction method?
  3. What are other methods of variance reduction?
  4. The theory of control variates tells us that our control variate should
    be correlated with the quantity we are trying to lower the variance of.
    Can we construct a better control variate that is even more correlated
    than a learned state-dependent value function? Why or why not?
     <details><summary>Hint</summary>
     <p>
     Right now, the typical control variate \( b(s) \) depends only on the state. Can we also have the control variate depend on the action? What extra work do we have to do to make sure this is okay? Check <a href="https://arxiv.org/abs/1611.02247">this paper</a> if you're interested in one way to extend this, and <a href="https://arxiv.org/abs/1802.10031">this paper</a> if you're interested in why adding dependence on more than just the state can be tricky and hard to implement in practice.
     </p>
     </details>
  5. We use control variates as a method to reduce variance in our gradient
    estimate. Why don't we use these for supervised learning problems, like
    classification? Or, are we implicitly using them?
     <details><summary>Solution</summary>
     <p>
      Reducing variance in our gradient estimates seems like an important thing to do, but we don't often see explicit variance reduction methods when we do supervised learning. However, there is a line of work around <b>stochastic variance reduced gradient</b> descent called <a href="https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf">SVRG</a> that try to construct gradient estimators that have reduced variance. See <a href="http://ranger.uta.edu/~heng/CSE6389_15_slides/SGD2.pdf">these slides</a> and <a href="https://arxiv.org/abs/1202.6258">these</a> <a href="https://arxiv.org/abs/1209.1873">papers</a> for more related work around this topic.
     </p>
     <p>
     However, the reason that we don't often see these being used in the supervised learning setting is because we're not necessarily looking to reduce the variance of SGD and converge smoothly to the minima of our training objective. This is because we're actually interested in looking for minima that have low <b>generalization error</b> and don't want to overfit to solutions with very small training error. In fact, we usually the consider the noise introduced by using minibatches in SGD as a good thing so that we can still escape minima that we may prematurely find. However, SVRG at its related algorithms are still very useful for solving optimization problems. Note that in reinforcement learning, we usually find that the variance of our gradient estimates is so high that it becomes one of the most important problems to solve first.
     </p>
      <p>
      Beyond supervised learning, control variates are used often in problems of Monte Carlo integration, which is used ubiquitously throughout Bayesian methods, and used for problems in hard attention, discrete latent random variables, and general stochastic computation graphs.
      </p>
     </details>

<br />

# 3 Fisher Information Matrix and Natural Gradient Descent
  <img src="/assets/fisher-steepest.png" />

  **Motivation**: While gradient descent is able to solve many optimization problems,
  it suffers from a basic problem - performance is dependent on the model's parameterization.
  Natural gradient descent, on the other hand, is invariant to model parameterization.
  This is achieved by multiplying gradient vectors by the inverse of the Fisher
  Information Matrix, which is a measure of how much model predictions change with
  local parameter changes.

  **Topics**:
  1. Fisher information matrix.
  2. Natural gradient descent.
  3. (Optional) K-Fac.

  **Required Readings**:
  1. [Matt Johnson’s Natural Gradient Descent and K-Fac Tutorial](/assets/k-fac-tutorial.pdf): Sections 1-7, Section A, B
  2. [New insights and perspectives on the natural gradient method](https://arxiv.org/pdf/1412.1193.pdf): Sections 1-11.
  3. [Fisher Information Matrix](https://web.archive.org/web/20170807004738/https://hips.seas.harvard.edu/blog/2013/04/08/fisher-information/)

  **Optional Readings**:
  1. [8-page intro to natural gradients](http://ipvs.informatik.uni-stuttgart.de/mlr/wp-content/uploads/2015/01/mathematics_for_intelligent_systems_lecture12_notes_I.pdf)
  2. [Why Natural Gradient Descent / Amari and Douglas](http://www.yaroslavvb.com/papers/amari-why.pdf)
  3. [Natural Gradient Works Efficiently in Learning / Amari](https://personalrobotics.ri.cmu.edu/files/courses/papers/Amari1998a.pdf)

  **Questions**:
  1. Consider classifiers $$p(y | x; \theta_{1})$$ and $$p(y | x; \theta_{2})$$,
     such that $$l_2(\theta_1, \theta_2)$$ is large. Does this imply difference
     in accuracy of the classifiers is high?
     <details><summary>Solution</summary>
     The accuracy of the classifier depends on the function defined by
     \(p(y|x;\theta) \). The distance between the parameters do not inform us
     about distance between the two functions. Hence, we cannot draw any conclusions
     about the difference in accuracy of the classifiers.
     </details>
  2. How is the Fisher matrix similar and different from the Hessian?
  3. How does natural gradient descent compare to Newton’s method?
  4. Why is the natural gradient slow to compute?
  5. How can one efficiently compute the product of the Fisher information matrix with an arbitrary vector?

<br />

# 4 Conjugate Gradient
  **Motivation**: The conjugate gradient method (CG) is an iterative algorithm for finding
  approximate solutions to $$Ax=b$$, where $$A$$ is a symmetric and positive-definite matrix (such
  as the Fisher information matrix). The method works by iteratively computing matrix-vector
  products $$Ax_i$$ and is particularly well-suited for matrices for which matrix-vector
  products are easy to compute.

  **Topics**:
  1. Solving system of linear equations.
  2. Efficiently computing matrix-vector products .
  3. Computational complexities of 2nd order methods.

  **Required Readings**:
  1. [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf): Section 7-9.
  2. [Convex Optimization and
     Approximation](https://ee227c.github.io/notes/ee227c-notes.pdf), UC Berkeley, Section 7.4
  3. Convex Optimization II by Stephen Boyd:
      1. [Lecture 12, from 37:10 to 1:05:00](https://www.youtube.com/watch?feature=player_embedded&v=cHVpwyYU_LY#t=2230)
      2. [Lecture 13, from 21:20 to 29:30](https://www.youtube.com/watch?feature=player_embedded&v=E4gl91l0l40#t=1266)

  **Optional Readings**:
  1. Numerical Optimization by Nocedal and Wright; Section 5.1, pages 101-120.
  2. [Metacademy](https://metacademy.org/graphs/concepts/conjugate_gradient)

  **Questions**:
  1. Remember that the natural gradient of a model is the Fisher information matrix of that model times the
     vanilla gradient ($$F^{-1}g$$). How does CG allow us to approximate the natural gradient?
      1. What is the naive way to compute $$F^{-1}g$$? How much memory and time would it take?
         <details><summary>Solution</summary>
         Assume we have an estimate of \(F\) (by the process
         in section 7 of <a href="/assets/k-fac-tutorial.pdf">Matt Johnson's tutorial</a>).
         Storing \(F\) would take space proportional to \(n^2\), and inverting \(F\) would take
         time proportional to \(n^3\) (or slightly lower with the Strassen algorithm)
         </details>
      2. How long would a run of CG to *exactly* compute $$F^{-1}g$$ take? How does that compare
         to the naive process?
         <details><summary>Solution</summary>
         Each iteration of CG computes \(Fv\) for some vector \(v\), which would take time
         proportional to \(n^2\). CG converges to the true answer after \(n\) steps, so in total
         it would take time proportional to \(n^3\). This process ends up being slower than directly inverting
         the Fisher naively and uses the same amount of memory.
         </details>
      3. How can we use CG and bring down the time and memory to compute the natural gradient $$F^{-1}g$$?
         <details><summary>Solution</summary>
         <ol>
           <li>
             Use the closed form estimate of \(Fv\) for arbitrary \(v\), as described in section A of <a href="/assets/k-fac-tutorial.pdf">Matt Johnson's tutorial</a>)
           </li>
           <li>
             Take fewer CG iteration steps, which leads to an approximation of the natural gradient that may be sufficient.
           </li>
         </ol>
  2. In pre-conditioned conjugate gradient, how does scaling the pre-conditioner
     matrix $$M$$ by a constant $$c$$ impact the convergence?
  3. Exercises 5.1 to 5.10 in Chapter 5, Numerical Optimization.
     (<b> Exercises 5.2, 5.9 are particularly recommended </b>)

<br />

# 5 Trust Region Methods
  **Motivation**: Trust region methods are a class of methods used in general
  optimization problems to constrain the update size. While
  TRPO does not use the full gamut of tools from the trust region literature,
  studying them provides good intuition for the problem that TRPO
  addresses and how we might improve the algorithm even more. In this
  section, we focus on understanding trust regions and line search methods.

  **Topics**:
  1. Trust regions and subproblems.
  2. Line search methods.

  **Required Readings**:
  1. [A friendly introduction to Trust Region Methods](https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods)
  2. Numerical Optimization by Nocedal and Wright: Chapter 2, Chapter 4, Section 4.1, 4.2

  **Optional Readings**:
  1. Numerical Optimization by Nocedal and Wright: Chapter 4, Section 4.3

  **Questions**:
  1. Instead of directly imposing constraints on the updates, what would be
     alternatives to enforce an algorithm to make bounded updates?
  2. Each step of a Trust Region optimization method updates parameters to
     the optimal setting given some constraint. Can we solve this in closed
     form using Lagrange Multipliers? In what way would this be similar, or
     different, from the trust region methods we just discussed?
  3. Exercises 4.1 to 4.10 in Chapter 4, Numerical Optimization.
     (<b>Exercise 4.10 is particularly recommended</b>)

<br />

# 6 The Paper
  **Motivation**: Let's read the [paper](https://arxiv.org/abs/1502.05477).
  We've built a good foundation for the various tools and mathematical ideas
  used by TRPO. In this section, we focus on the parts of the paper that aren't
  explicitly covered by the above topics and together result in the practical
  algorithm used by many today. These are monotonic policy improvement and the
  two different implementation approaches: vine and single-path.

  **Topics**:
  1. What is the problem with policy gradients that TRPO addresses?
  2. What are the bottlenecks to addressing that problem in the existing approaches when it debuted?
  3. Policy improvement bounds and theory.

  **Required Readings**:
  1. [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)
  2. Deep RL Course at UC Berkeley (CS 294); Advanced Policy Gradient Methods (TRPO)
      1. [Slides](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf)
      2. [Video](https://www.youtube.com/watch?v=ycCtmp4hcUs&feature=youtu.be&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3)
  3. [Approximately Optimal Approximate Reinforcement Learning](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf)

  **Optional Readings**:
  2. [TRPO Tutorial](https://reinforce.io/blog/end-to-end-computation-graphs-for-reinforcement-learning/)
  3. [ACKTR](https://arxiv.org/abs/1708.05144)
  4. [A Natural Policy Gradient](https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)

  **Questions**:
  1. How is the trust region set in TRPO? Can we do better? Under what
     assumptions is imposing the trust region constraint not required?
  2. Why do we use conjugate gradient methods for optimization in TRPO? Can we
    exploit the fact the conjugate gradient optimization is differentiable?
  3. How is line search used in TRPO?
  4. How does TRPO differ from Natural Policy Gradient?
     <details><summary>Solution</summary>
     <p> See slides 30-34 from <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf">this lecture</a>. </p>
    </details>
  5. What are the pros and cons of using the vine and single-path methods?
  6. In practice, TRPO is really slow. What is the main computational
    bottleneck and how might we remove it? Can we approximate this bottleneck?
  7. TRPO makes a series of approximations that deviate from the policy
    improvement theory that is cited. What are the assumptions that are made
    that allow these approximations to be reasonable? Should we still expect
    monotonic improvement in our policy?
  8. TRPO is a general procedure to directly optimize parameters from rewards,
    even though the procedure is "non-differentiable". Does it make sense to
    apply TRPO to other non-differentiable problems, like problems involving
    hard attention or discrete random variables?
