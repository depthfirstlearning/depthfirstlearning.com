---
layout: audio
title:  "Characterising Bias in Compressed Models by Sara Hooker"
date:   2021-08-10 10:00:00 -0400
audiofile: /assets/audios/sarahooker.cbcm.mp3
captionfile: "captions/sarahooker.cbcm.html"
categories: generalization,compression
author: cinjon
blurb: "[Audio] Characterising Bias in Compressed Models is an important paper analyzing
        how the methods that we use to make machine learning models smaller impact
        certain subgroups more so than others. Hear Sara Hooker, the first author,
        talk about how this approach works, as well as her research motivation."
feedback: true
---

[Characterising Bias in Compressed Models](https://arxiv.org/abs/2010.03058) by [Sara Hooker](https://www.sarahooker.me/)
et al highlighted where the lunch was getting paid when it came to 
modern deep learning compression techniques. All of these models we use on a pervasive
basis, in her phones, on social feeds, etc, they all use compression. Are we 
compromising what we want when we apply this everywhere? More particularly, are
we affecting some groups more than others?
